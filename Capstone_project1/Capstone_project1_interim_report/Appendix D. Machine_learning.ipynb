{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix D. Machine learning: multilabel classification\n",
    "\n",
    "The dataset consists of labelled IR spectra. Each spectrum is assigned to one or more labels, and the labels are not mutually exclusive. Therefore, we use multilabel classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project 1: IR spectral analysis of organic compounds via a machine learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Table of contents**\n",
    "#### &nbsp;&nbsp; (I) Import and prepare dataset\n",
    "#### &nbsp;&nbsp; (II) Apply a dimensionality reduction technique (i.e., PCA)\n",
    "#### &nbsp;&nbsp; (III) Screen multiple models simultaneously\n",
    "#### &nbsp;&nbsp; (IV) Focus on one best method and improve model\n",
    "#### &nbsp;&nbsp; (V) Further improve accuracy with ensemble methods\n",
    "#### &nbsp;&nbsp; (VI) Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (I) Import and prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>600</th>\n",
       "      <th>604</th>\n",
       "      <th>608</th>\n",
       "      <th>612</th>\n",
       "      <th>616</th>\n",
       "      <th>620</th>\n",
       "      <th>624</th>\n",
       "      <th>628</th>\n",
       "      <th>632</th>\n",
       "      <th>...</th>\n",
       "      <th>3464</th>\n",
       "      <th>3468</th>\n",
       "      <th>3472</th>\n",
       "      <th>3476</th>\n",
       "      <th>3480</th>\n",
       "      <th>3484</th>\n",
       "      <th>3488</th>\n",
       "      <th>3492</th>\n",
       "      <th>3496</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1119-40-0</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.011373</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>0.012765</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.015177</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.014654</td>\n",
       "      <td>0.014541</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120-33-2</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>0.006625</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120-51-4</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120650-77-3</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.003658</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.003962</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.006065</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1335-40-6</td>\n",
       "      <td>0.026083</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.025201</td>\n",
       "      <td>0.024101</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>0.020080</td>\n",
       "      <td>0.016694</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005708</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0       600       604       608       612       616       620  \\\n",
       "0    1119-40-0  0.009480  0.011373  0.008950  0.010881  0.012741  0.012765   \n",
       "1     120-33-2  0.001862  0.002358  0.000965  0.001367  0.001221  0.002217   \n",
       "2     120-51-4  0.011749  0.009791  0.006795  0.006387  0.006919  0.005475   \n",
       "3  120650-77-3  0.004431  0.005630  0.005578  0.005711  0.004729  0.003658   \n",
       "4    1335-40-6  0.026083  0.025300  0.025201  0.024101  0.023793  0.022894   \n",
       "\n",
       "        624       628       632  ...        3464      3468      3472  \\\n",
       "0  0.011669  0.010218  0.008583  ...    0.007078  0.007871  0.009175   \n",
       "1  0.001162  0.000631  0.001026  ...    0.007558  0.006625  0.004902   \n",
       "2  0.004003  0.002257  0.002252  ...    0.005814  0.004950  0.004097   \n",
       "3  0.004980  0.003962  0.003673  ...    0.004496  0.005218  0.003233   \n",
       "4  0.020080  0.016694  0.013240  ...    0.005708  0.005478  0.005245   \n",
       "\n",
       "       3476      3480      3484      3488      3492      3496  label  \n",
       "0  0.010160  0.010963  0.015177  0.013968  0.014654  0.014541  ester  \n",
       "1  0.004450  0.003911  0.003702  0.003551  0.002494  0.002388  ester  \n",
       "2  0.003773  0.003183  0.002548  0.002194  0.002459  0.002356  ester  \n",
       "3  0.006065  0.004307  0.004863  0.005305  0.003419  0.005081  ester  \n",
       "4  0.005083  0.005195  0.005259  0.005337  0.005369  0.005188  ester  \n",
       "\n",
       "[5 rows x 727 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('data/NIST_selected_organic_spectra.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405, 727)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Apply multilabel  \n",
    "\n",
    "ref.: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html\n",
    "\n",
    "- The labels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ester', 'ketone', 'alcohol', 'alkane', 'alkene', 'amine',\n",
       "       'aldehyde', 'acid', 'halide', 'benzene'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the label names:\n",
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ref.: Brian C. Smith, \"Infrared Spectral Interpretation: A Systematic Approach\", CRC Press 1998.  \n",
    "#we have ten labels:  \n",
    "#Note: Aldehydes have the characteristic C-H stretch, denoted here as C-H_ald to differentiate from ketones.  \n",
    "\n",
    "label_names=  \n",
    "['C-H', 'C=C', \\  #1,2    \n",
    "'C=O', 'C-O', \\  #3,4  \n",
    "'O-H', 'C-N', \\  #5,6  \n",
    "'N-H', 'C-X', \\  #7,8  \n",
    "'Ar', 'C-H_ald'] #9,10    \n",
    "\n",
    "Cheat-sheet:  \n",
    "    ester=(1,3,4,,,,,,)  \n",
    "    ketone=(1,3,,,,,,,)  \n",
    "    alcohol=(1,4,,,,,,,)  \n",
    "    alkane=(1,2,,,,,,,)  \n",
    "    alkene=(1,2,,,,,,,)  \n",
    "    amine=(1,6,7,,,,,,)  \n",
    "    aldehyde=(1,3,10,,,,,,)  \n",
    "    alcid=(1,3,4,5,,,,,)  \n",
    "    halide=(1,8,,,,,,,)  \n",
    "    benzene=(1,2,9,,,,,,)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. define labels for each functional group as a list.\n",
    "ester_label=[1,3,4]  \n",
    "ketone_label=[1,3]  \n",
    "alcohol_label=[1,4]  \n",
    "alkane_label=[1,2]  \n",
    "alkene_label=[1,2]  \n",
    "amine_label=[1,6,7]  \n",
    "aldehyde_label=[1,3,10]  \n",
    "acid_label=[1,3,4,5]  \n",
    "halide_label=[1,8]  \n",
    "benzene_label=[1,2,9]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [1, 1, 0, ..., 0, 1, 0],\n",
       "       [1, 1, 0, ..., 0, 1, 0],\n",
       "       [1, 1, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create labels\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer() #create object\n",
    "\n",
    "#generate as many labels as there are entries in the respective groups and the combine it into a single array with row number matching with the Dataset\n",
    "#https://stackoverflow.com/questions/19753279/repeat-a-tuple-inside-a-tuple\n",
    "y_label=len(df[df.label=='ester'])*[ester_label,] + \\\n",
    "        len(df[df.label=='ketone'])*[ketone_label,] + \\\n",
    "        len(df[df.label=='alcohol'])*[alcohol_label,] + \\\n",
    "        len(df[df.label=='alkane'])*[alkane_label,] + \\\n",
    "        len(df[df.label=='alkene'])*[alkene_label,] + \\\n",
    "        len(df[df.label=='amine'])*[amine_label,] + \\\n",
    "        len(df[df.label=='aldehyde'])*[aldehyde_label,] + \\\n",
    "        len(df[df.label=='acid'])*[acid_label,] + \\\n",
    "        len(df[df.label=='halide'])*[halide_label,] + \\\n",
    "        len(df[df.label=='benzene'])*[benzene_label,] \n",
    "\n",
    "y_label=mlb.fit_transform(y_label) #convert tuples list to multilabelbinarizer\n",
    "#len(y_label) #check\n",
    "y_label #check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (II) Screen various techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. split datasets into train/test datasets for PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    df.iloc[:,1:726], y_label, test_size=0.2, random_state=13)\n",
    "\n",
    "#2. normalize datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler().fit(X_train)\n",
    "standardized_X=scaler.transform(X_train)\n",
    "standardized_X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC: 0.580208 (0.065486)\n",
      "ETC: 0.518371 (0.081575)\n",
      "ETC_E: 0.577273 (0.094651)\n",
      "KNC: 0.601610 (0.060275)\n",
      "RFC: 0.556061 (0.088131)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23f9a44ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('DTC', DecisionTreeClassifier())) \n",
    "models.append(('ETC', ExtraTreeClassifier()))\n",
    "models.append(('ETC_E', ExtraTreesClassifier()))\n",
    "models.append(('KNC', KNeighborsClassifier()))\n",
    "models.append(('RFC', RandomForestClassifier()))\n",
    "\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "\n",
    "for name, model in models: #iterate through each of the different models\n",
    "    #Use k-fold validation to determine model accuracy. k-fold validation is one of many ways, but it is the gold standard way.\n",
    "    try:\n",
    "        kfold = KFold(n_splits=10, random_state=7) #initiate k-fold validation. Here, we use 10-folds.\n",
    "        cv_results = cross_val_score(model, standardized_X, y_train, cv=kfold, scoring=scoring) #apply model in turns.\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "    except:\n",
    "        print('Error in:', name, 'and', model)\n",
    "        \n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  It is seen that ExtraTreesClassifier and KNeighborsClassifier yield the highest score of about 0.60 and a narrow standard deviation.\n",
    "- Next, I will utilize dimensionality reduction technique and simultaneously perform a hyperparameter tuning on these two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (III) Apply a dimensionality reduction technique (i.e., PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv+C7fIpSEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23f9a3ce198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Perform pca\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "no_components=100\n",
    "\n",
    "pca=PCA(n_components=no_components) #generate model\n",
    "\n",
    "X_pca=pca.fit_transform(df.iloc[:,1:726])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(np.arange(no_components),100*pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('% Variation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above plot shows that n_component=100 is sufficient to capture close to 100% of all variations. Below, we will carry out hyperparameter tuning to determine the optimum parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (IV) Optimize the parameters for ExtraTreesClassifier and KNeighborsClassifier, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. split datasets into train/test datasets for PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split( \n",
    "    X_pca, y_label, test_size=0.2, random_state=13)\n",
    "\n",
    "#2. scale datasets\n",
    "scaler=StandardScaler().fit(X_train_pca)\n",
    "standardized_X_pca=scaler.transform(X_train_pca)\n",
    "standardized_X_test_pca=scaler.transform(X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) ExtraTreesClassifier\n",
    "- Tune n_component and max_depth simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the best parameters, please wait...\n",
      "\n",
      "\n",
      "Best score: ('components = ', 20, ', estimators = ', 200, ', Score = ', 0.62962962962962965)\n"
     ]
    }
   ],
   "source": [
    "#Let's tune the knn n_neighbors parameter\n",
    "components = [5, 10, 15, 20, 25, 30, 50, 100] #optimize using up to 100 components\n",
    "estimators = [10, 50, 100, 150, 200, 250] #and using up to 7 neighbors\n",
    "\n",
    "ets_score=0 #temporarily stores previous score to be used to compare current with previous score\n",
    "ets_best=\" \" #a list for storing the best parameter for the knn model\n",
    "\n",
    "print('Finding the best parameters, please wait...')\n",
    "\n",
    "for component in components:\n",
    "    for n in estimators:\n",
    "        etc = ExtraTreesClassifier(n_estimators=n, random_state=0)\n",
    "        etc.fit(standardized_X_pca[:,:component], y_train_pca)\n",
    "        score = etc.score(standardized_X_test_pca[:,:component], y_test)\n",
    "        \n",
    "        if score>ets_score:\n",
    "            ets_score=score\n",
    "            ets_best='components = ', component, ', estimators = ', n,', Score = ', score\n",
    "        \n",
    "        #print('components = ', component, ', estimators = ', n,', Score = ', score)  \n",
    "\n",
    "            \n",
    "print('\\n')\n",
    "print('Best score:', ets_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The highest score is 63% (components =  20 , estimators =  200). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) KNeighborsClassifier\n",
    "- Tune n_component and n_neighbor simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the best parameters, please wait...\n",
      "\n",
      "\n",
      "Best score: ('components = ', 15, ', n = ', 3, ', Score = ', 0.69135802469135799)\n"
     ]
    }
   ],
   "source": [
    "#Let's tune the knn n_neighbors parameter\n",
    "neighbors = [1, 2, 3, 4, 5, 6, 7] #and using up to 7 neighbors\n",
    "\n",
    "knn_score=0 #temporarily stores previous score to be used to compare current with previous score\n",
    "knn_best=\" \" #a list for storing the best parameter for the knn model\n",
    "\n",
    "print('Finding the best parameters, please wait...')\n",
    "\n",
    "for component in components:\n",
    "    for n in neighbors:\n",
    "        knn = KNeighborsClassifier(n_neighbors=n)\n",
    "        knn.fit(standardized_X_pca[:,:component], y_train_pca)\n",
    "        score = knn.score(standardized_X_test_pca[:,:component], y_test)\n",
    "        \n",
    "        if score>knn_score:\n",
    "            knn_score=score\n",
    "            knn_best='components = ', component, ', n = ', n,', Score = ', score\n",
    "        \n",
    "        #print('components = ', component, ', neighbors = ', n,', Score = ', score)  \n",
    "        \n",
    "print('\\n')\n",
    "print('Best score:', knn_best)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The highest score is 69% (components =  15 , neighbors = 3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (V) Further improve accuracy with ensemble methods\n",
    "\n",
    "ref.: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Ensemble: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the best parameters, please wait...\n",
      "\n",
      "\n",
      "best_params: ('components: ', 10, 'est:', 11) and score: 0.654320987654\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_score=0\n",
    "rfc_params=\" \"\n",
    "\n",
    "print('Finding the best parameters, please wait...')\n",
    "\n",
    "for c in range(5,100,5):\n",
    "    for est in range(1,30,1):\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=est)\n",
    "        clf = clf.fit(standardized_X_pca[:,:c], y_train_pca)\n",
    "        score=clf.score(standardized_X_test_pca[:,:c], y_test)\n",
    "        #print('score:', score)\n",
    "        \n",
    "        if score>rfc_score:\n",
    "            rfc_score=score\n",
    "            rfc_params='components: ', c, 'est:', est\n",
    "print('\\n')\n",
    "print('best_params:', rfc_params, 'and score:', rfc_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best score is 65% (components=10 and n_estimators=29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Ensemble: ExtratreesClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the best parameters, please wait...\n",
      "\n",
      "\n",
      "best_params: ('components: ', 10, 'n_estimators:', 9) and score: 0.641975308642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ets_score=0\n",
    "ets_params=\" \"\n",
    "\n",
    "print('Finding the best parameters, please wait...')\n",
    "\n",
    "for c in range(5,100,5):\n",
    "    for est in range(1,30,1):\n",
    "\n",
    "        clf = ExtraTreesClassifier(n_estimators=est)\n",
    "        clf = clf.fit(standardized_X_pca[:,:c], y_train_pca)\n",
    "        score=clf.score(standardized_X_test_pca[:,:c], y_test)\n",
    "        \n",
    "        if score>ets_score:\n",
    "            ets_score=score\n",
    "            ets_params='components: ', c, 'n_estimators:', est\n",
    "print('\\n')\n",
    "print('best_params:', ets_params, 'and score:', ets_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best score is 64% (components=15, n_estimators=9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (VI) Estimate score uncertainty with k-fold cross-validation\n",
    "\n",
    "The best found scores is obtained for KNeighborsClassifier (69%). To determine the score uncertainty, I carry out k-fold cross-validation using the best-found parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KNeighborsClassifier using k-fold cross-validation \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#combine scaled datasets\n",
    "X=np.concatenate((standardized_X_pca,standardized_X_test_pca),axis=0)\n",
    "Y=np.concatenate((y_train_pca,y_test_pca),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.896% (5.145%)\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "component=15\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=n)\n",
    "results = cross_val_score(knn, X[:,:component], Y, cv=kfold)\n",
    "\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Therefore, the score accuracy based on KNeighborsClassifier model is 69.896 ± 5.145%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (VII) Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Five machine learning models that are suitable for multilabel classification problems are assessed (i.e., DecisionTreeClassifier, ExtraTreeClassifier, ExtraTreesClassifier,  KNeighborsClassifier, and RandomForestClassifier).\n",
    "- Initial screening reveals that ExtraTreesClassifier and KNeighborsClassifier perform better than the other three algorithms. \n",
    "- Next, I utilize dimensionality reduction techniques (i.e., PCA) along with the two machine learning algorithms and optimize the parameters in the respective models. \n",
    "- Using KNeighborsClassifier, the best score that is achieved is 69.0% (Components = 20 , neighbors = 5). This is a very modest score. Ensemble methods are attempted (i.e., RandomForestClassifier and ExtraTreesClassifier), but the scores did not exceed 69.0%.  \n",
    "- By carrying out k-fold cross-validation, the uncertainty in the score for KNeighborsClassifier is 69.896 ± 5.145%.\n",
    "- To further improve the model accuracy, I will consider a neural network model.   \n",
    "- First, I will attempt a basic neural network algorithm to achieve an accuracy above 90%. \n",
    "- If it is not achievable, I will consider employing a convolutional neural network algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
