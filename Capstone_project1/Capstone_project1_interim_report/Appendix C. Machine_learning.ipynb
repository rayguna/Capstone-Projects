{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix C. Machine learning: multilabel classification\n",
    "\n",
    "The dataset consists of labelled IR spectra. Each spectrum is assigned to one or more labels, and the labels are not mutually exclusive. Therefore, we use multilabel classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project 1: IR spectral analysis of organic compounds via a machine learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Table of contents**\n",
    "#### &nbsp;&nbsp; (I) Import and prepare dataset\n",
    "#### &nbsp;&nbsp; (II) Apply a dimensionality reduction technique (i.e., PCA)\n",
    "#### &nbsp;&nbsp; (III) Screen multiple models simultaneously\n",
    "#### &nbsp;&nbsp; (IV) Focus on one best method and improve model\n",
    "#### &nbsp;&nbsp; (V) Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (I) Import and prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>600</th>\n",
       "      <th>604</th>\n",
       "      <th>608</th>\n",
       "      <th>612</th>\n",
       "      <th>616</th>\n",
       "      <th>620</th>\n",
       "      <th>624</th>\n",
       "      <th>628</th>\n",
       "      <th>632</th>\n",
       "      <th>...</th>\n",
       "      <th>3464</th>\n",
       "      <th>3468</th>\n",
       "      <th>3472</th>\n",
       "      <th>3476</th>\n",
       "      <th>3480</th>\n",
       "      <th>3484</th>\n",
       "      <th>3488</th>\n",
       "      <th>3492</th>\n",
       "      <th>3496</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1119-40-0</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.011373</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>0.012765</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.015177</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.014654</td>\n",
       "      <td>0.014541</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120-33-2</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>0.006625</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120-51-4</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120650-77-3</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.003658</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.003962</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.006065</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1335-40-6</td>\n",
       "      <td>0.026083</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.025201</td>\n",
       "      <td>0.024101</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>0.020080</td>\n",
       "      <td>0.016694</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005708</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>ester</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0       600       604       608       612       616       620  \\\n",
       "0    1119-40-0  0.009480  0.011373  0.008950  0.010881  0.012741  0.012765   \n",
       "1     120-33-2  0.001862  0.002358  0.000965  0.001367  0.001221  0.002217   \n",
       "2     120-51-4  0.011749  0.009791  0.006795  0.006387  0.006919  0.005475   \n",
       "3  120650-77-3  0.004431  0.005630  0.005578  0.005711  0.004729  0.003658   \n",
       "4    1335-40-6  0.026083  0.025300  0.025201  0.024101  0.023793  0.022894   \n",
       "\n",
       "        624       628       632  ...        3464      3468      3472  \\\n",
       "0  0.011669  0.010218  0.008583  ...    0.007078  0.007871  0.009175   \n",
       "1  0.001162  0.000631  0.001026  ...    0.007558  0.006625  0.004902   \n",
       "2  0.004003  0.002257  0.002252  ...    0.005814  0.004950  0.004097   \n",
       "3  0.004980  0.003962  0.003673  ...    0.004496  0.005218  0.003233   \n",
       "4  0.020080  0.016694  0.013240  ...    0.005708  0.005478  0.005245   \n",
       "\n",
       "       3476      3480      3484      3488      3492      3496  label  \n",
       "0  0.010160  0.010963  0.015177  0.013968  0.014654  0.014541  ester  \n",
       "1  0.004450  0.003911  0.003702  0.003551  0.002494  0.002388  ester  \n",
       "2  0.003773  0.003183  0.002548  0.002194  0.002459  0.002356  ester  \n",
       "3  0.006065  0.004307  0.004863  0.005305  0.003419  0.005081  ester  \n",
       "4  0.005083  0.005195  0.005259  0.005337  0.005369  0.005188  ester  \n",
       "\n",
       "[5 rows x 727 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('data/NIST_selected_organic_spectra.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405, 727)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Apply multilabel  \n",
    "\n",
    "ref.: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html\n",
    "\n",
    "- The labels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ester', 'ketone', 'alcohol', 'alkane', 'alkene', 'amine',\n",
       "       'aldehyde', 'acid', 'halide', 'benzene'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the label names:\n",
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ref.: Brian C. Smith, \"Infrared Spectral Interpretation: A Systematic Approach\", CRC Press 1998.  \n",
    "#we have ten labels:  \n",
    "#Note: Aldehydes have the characteristic C-H stretch, denoted here as C-H_ald to differentiate from ketones.  \n",
    "\n",
    "label_names=  \n",
    "['C-H', 'C=C', \\  #1,2    \n",
    "'C=O', 'C-O', \\  #3,4  \n",
    "'O-H', 'C-N', \\  #5,6  \n",
    "'N-H', 'C-X', \\  #7,8  \n",
    "'Ar', 'C-H_ald'] #9,10    \n",
    "\n",
    "Cheat-sheet:  \n",
    "    ester=(1,3,4,,,,,,)  \n",
    "    ketone=(1,3,,,,,,,)  \n",
    "    alcohol=(1,4,,,,,,,)  \n",
    "    alkane=(1,2,,,,,,,)  \n",
    "    alkene=(1,2,,,,,,,)  \n",
    "    amine=(1,6,7,,,,,,)  \n",
    "    aldehyde=(1,3,10,,,,,,)  \n",
    "    alcid=(1,3,4,5,,,,,)  \n",
    "    halide=(1,8,,,,,,,)  \n",
    "    benzene=(1,2,9,,,,,,)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. define labels for each functional group as a list.\n",
    "ester_label=[1,3,4]  \n",
    "ketone_label=[1,3]  \n",
    "alcohol_label=[1,4]  \n",
    "alkane_label=[1,2]  \n",
    "alkene_label=[1,2]  \n",
    "amine_label=[1,6,7]  \n",
    "aldehyde_label=[1,3,10]  \n",
    "acid_label=[1,3,4,5]  \n",
    "halide_label=[1,8]  \n",
    "benzene_label=[1,2,9]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [1, 1, 0, ..., 0, 1, 0],\n",
       "       [1, 1, 0, ..., 0, 1, 0],\n",
       "       [1, 1, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create labels\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer() #create object\n",
    "\n",
    "#generate as many labels as there are entries in the respective groups and the combine it into a single array with row number matching with the Dataset\n",
    "#https://stackoverflow.com/questions/19753279/repeat-a-tuple-inside-a-tuple\n",
    "y_label=len(df[df.label=='ester'])*[ester_label,] + \\\n",
    "        len(df[df.label=='ketone'])*[ketone_label,] + \\\n",
    "        len(df[df.label=='alcohol'])*[alcohol_label,] + \\\n",
    "        len(df[df.label=='alkane'])*[alkane_label,] + \\\n",
    "        len(df[df.label=='alkene'])*[alkene_label,] + \\\n",
    "        len(df[df.label=='amine'])*[amine_label,] + \\\n",
    "        len(df[df.label=='aldehyde'])*[aldehyde_label,] + \\\n",
    "        len(df[df.label=='acid'])*[acid_label,] + \\\n",
    "        len(df[df.label=='halide'])*[halide_label,] + \\\n",
    "        len(df[df.label=='benzene'])*[benzene_label,] \n",
    "\n",
    "y_label=mlb.fit_transform(y_label) #convert tuples list to multilabelbinarizer\n",
    "#len(y_label) #check\n",
    "y_label #check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (II) Screen various techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. split datasets into train/test datasets for PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    df.iloc[:,1:726], y_label, test_size=0.2, random_state=13)\n",
    "\n",
    "#2. normalize datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler().fit(X_train)\n",
    "standardized_X=scaler.transform(X_train)\n",
    "standardized_X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR: 0.564867 (0.062327)\n",
      "ETC: 0.531345 (0.126103)\n",
      "ETC_E: 0.589489 (0.076468)\n",
      "KNC: 0.601610 (0.060275)\n",
      "RFC: 0.509754 (0.076019)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmYJcribmSXI4W5mliCHu5lZgv4fSbjXB1ifke8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2430ad2e5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('DTR', DecisionTreeClassifier())) \n",
    "models.append(('ETC', ExtraTreeClassifier()))\n",
    "models.append(('ETC_E', ExtraTreesClassifier()))\n",
    "models.append(('KNC', KNeighborsClassifier()))\n",
    "models.append(('RFC', RandomForestClassifier()))\n",
    "\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "\n",
    "for name, model in models: #iterate through each of the different models\n",
    "    #Use k-fold validation to determine model accuracy. k-fold validation is one of many ways, but it is the gold standard way.\n",
    "    try:\n",
    "        kfold = KFold(n_splits=10, random_state=7) #initiate k-fold validation. Here, we use 10-folds.\n",
    "        cv_results = cross_val_score(model, standardized_X, y_train, cv=kfold, scoring=scoring) #apply model in turns.\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "    except:\n",
    "        print('Error in:', name, 'and', model)\n",
    "        \n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  It is seen that ExtraTreesClassifier and KNeighborsClassifier yield the highest score of about 0.60 and a narrow standard deviation.\n",
    "- Next, I will utilize dimensionality reduction technique and simultaneously perform a hyperparameter tuning on these two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (III) Apply a dimensionality reduction technique (i.e., PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns324OSAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24309b35f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Perform pca\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "no_components=100\n",
    "\n",
    "pca=PCA(n_components=no_components) #generate model\n",
    "\n",
    "X_pca=pca.fit_transform(df.iloc[:,1:726])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(np.arange(no_components),100*pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('% Variation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above plot shows that n_component=100 is sufficient to capture close to 100% of all variations. Below, we will carry out hyperparameter tuning to determine the optimum parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (IV) Optimize the parameters for ExtraTreesClassifier and KNeighborsClassifier, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. split datasets into train/test datasets for PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split( \n",
    "    X_pca, y_label, test_size=0.2, random_state=13)\n",
    "\n",
    "#2. normalize datasets\n",
    "\n",
    "scaler=StandardScaler().fit(X_train_pca)\n",
    "standardized_X_pca=scaler.transform(X_train_pca)\n",
    "standardized_X_test_pca=scaler.transform(X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) ExtraTreesClassifier\n",
    "- Tune n_component and max_depth simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components =  5 , estimators =  10 , Score =  0.493827160494\n",
      "Components =  5 , estimators =  50 , Score =  0.530864197531\n",
      "Components =  5 , estimators =  100 , Score =  0.543209876543\n",
      "Components =  5 , estimators =  150 , Score =  0.493827160494\n",
      "Components =  5 , estimators =  200 , Score =  0.506172839506\n",
      "Components =  5 , estimators =  250 , Score =  0.506172839506\n",
      "Components =  10 , estimators =  10 , Score =  0.555555555556\n",
      "Components =  10 , estimators =  50 , Score =  0.567901234568\n",
      "Components =  10 , estimators =  100 , Score =  0.567901234568\n",
      "Components =  10 , estimators =  150 , Score =  0.555555555556\n",
      "Components =  10 , estimators =  200 , Score =  0.555555555556\n",
      "Components =  10 , estimators =  250 , Score =  0.555555555556\n",
      "Components =  15 , estimators =  10 , Score =  0.493827160494\n",
      "Components =  15 , estimators =  50 , Score =  0.543209876543\n",
      "Components =  15 , estimators =  100 , Score =  0.555555555556\n",
      "Components =  15 , estimators =  150 , Score =  0.592592592593\n",
      "Components =  15 , estimators =  200 , Score =  0.567901234568\n",
      "Components =  15 , estimators =  250 , Score =  0.592592592593\n",
      "Components =  20 , estimators =  10 , Score =  0.543209876543\n",
      "Components =  20 , estimators =  50 , Score =  0.604938271605\n",
      "Components =  20 , estimators =  100 , Score =  0.592592592593\n",
      "Components =  20 , estimators =  150 , Score =  0.58024691358\n",
      "Components =  20 , estimators =  200 , Score =  0.62962962963\n",
      "Components =  20 , estimators =  250 , Score =  0.567901234568\n",
      "Components =  25 , estimators =  10 , Score =  0.543209876543\n",
      "Components =  25 , estimators =  50 , Score =  0.567901234568\n",
      "Components =  25 , estimators =  100 , Score =  0.604938271605\n",
      "Components =  25 , estimators =  150 , Score =  0.592592592593\n",
      "Components =  25 , estimators =  200 , Score =  0.58024691358\n",
      "Components =  25 , estimators =  250 , Score =  0.592592592593\n",
      "Components =  30 , estimators =  10 , Score =  0.481481481481\n",
      "Components =  30 , estimators =  50 , Score =  0.555555555556\n",
      "Components =  30 , estimators =  100 , Score =  0.567901234568\n",
      "Components =  30 , estimators =  150 , Score =  0.567901234568\n",
      "Components =  30 , estimators =  200 , Score =  0.567901234568\n",
      "Components =  30 , estimators =  250 , Score =  0.567901234568\n",
      "Components =  50 , estimators =  10 , Score =  0.432098765432\n",
      "Components =  50 , estimators =  50 , Score =  0.481481481481\n",
      "Components =  50 , estimators =  100 , Score =  0.493827160494\n",
      "Components =  50 , estimators =  150 , Score =  0.530864197531\n",
      "Components =  50 , estimators =  200 , Score =  0.518518518519\n",
      "Components =  50 , estimators =  250 , Score =  0.530864197531\n",
      "Components =  100 , estimators =  10 , Score =  0.345679012346\n",
      "Components =  100 , estimators =  50 , Score =  0.456790123457\n",
      "Components =  100 , estimators =  100 , Score =  0.469135802469\n",
      "Components =  100 , estimators =  150 , Score =  0.481481481481\n",
      "Components =  100 , estimators =  200 , Score =  0.469135802469\n",
      "Components =  100 , estimators =  250 , Score =  0.444444444444\n"
     ]
    }
   ],
   "source": [
    "#Let's tune the knn n_neighbors parameter\n",
    "components = [5, 10, 15, 20, 25, 30, 50, 100] #optimize using up to 100 components\n",
    "estimators = [10, 50, 100, 150, 200, 250] #and using up to 7 neighbors\n",
    "\n",
    "\n",
    "for component in components:\n",
    "    for n in estimators:\n",
    "        etc = ExtraTreesClassifier(n_estimators=n, random_state=0)\n",
    "        etc.fit(standardized_X_pca[:,:component], y_train)\n",
    "        score = etc.score(standardized_X_test_pca[:,:component], y_test)\n",
    "        \n",
    "        print('Components = ', component, ', estimators = ', n,', Score = ', score)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The highest score is 63% (Components =  20 , estimators =  200). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) KNeighborsClassifier\n",
    "- Tune n_component and n_neighbor simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components =  5 , neighbors =  1 , Score =  0.567901234568\n",
      "Components =  5 , neighbors =  2 , Score =  0.444444444444\n",
      "Components =  5 , neighbors =  3 , Score =  0.506172839506\n",
      "Components =  5 , neighbors =  4 , Score =  0.456790123457\n",
      "Components =  5 , neighbors =  5 , Score =  0.481481481481\n",
      "Components =  5 , neighbors =  6 , Score =  0.444444444444\n",
      "Components =  5 , neighbors =  7 , Score =  0.469135802469\n",
      "Components =  10 , neighbors =  1 , Score =  0.666666666667\n",
      "Components =  10 , neighbors =  2 , Score =  0.604938271605\n",
      "Components =  10 , neighbors =  3 , Score =  0.654320987654\n",
      "Components =  10 , neighbors =  4 , Score =  0.62962962963\n",
      "Components =  10 , neighbors =  5 , Score =  0.62962962963\n",
      "Components =  10 , neighbors =  6 , Score =  0.555555555556\n",
      "Components =  10 , neighbors =  7 , Score =  0.555555555556\n",
      "Components =  15 , neighbors =  1 , Score =  0.641975308642\n",
      "Components =  15 , neighbors =  2 , Score =  0.592592592593\n",
      "Components =  15 , neighbors =  3 , Score =  0.691358024691\n",
      "Components =  15 , neighbors =  4 , Score =  0.592592592593\n",
      "Components =  15 , neighbors =  5 , Score =  0.641975308642\n",
      "Components =  15 , neighbors =  6 , Score =  0.604938271605\n",
      "Components =  15 , neighbors =  7 , Score =  0.617283950617\n",
      "Components =  20 , neighbors =  1 , Score =  0.641975308642\n",
      "Components =  20 , neighbors =  2 , Score =  0.567901234568\n",
      "Components =  20 , neighbors =  3 , Score =  0.654320987654\n",
      "Components =  20 , neighbors =  4 , Score =  0.641975308642\n",
      "Components =  20 , neighbors =  5 , Score =  0.691358024691\n",
      "Components =  20 , neighbors =  6 , Score =  0.617283950617\n",
      "Components =  20 , neighbors =  7 , Score =  0.617283950617\n",
      "Components =  25 , neighbors =  1 , Score =  0.592592592593\n",
      "Components =  25 , neighbors =  2 , Score =  0.58024691358\n",
      "Components =  25 , neighbors =  3 , Score =  0.641975308642\n",
      "Components =  25 , neighbors =  4 , Score =  0.617283950617\n",
      "Components =  25 , neighbors =  5 , Score =  0.641975308642\n",
      "Components =  25 , neighbors =  6 , Score =  0.58024691358\n",
      "Components =  25 , neighbors =  7 , Score =  0.58024691358\n",
      "Components =  30 , neighbors =  1 , Score =  0.617283950617\n",
      "Components =  30 , neighbors =  2 , Score =  0.567901234568\n",
      "Components =  30 , neighbors =  3 , Score =  0.617283950617\n",
      "Components =  30 , neighbors =  4 , Score =  0.567901234568\n",
      "Components =  30 , neighbors =  5 , Score =  0.58024691358\n",
      "Components =  30 , neighbors =  6 , Score =  0.506172839506\n",
      "Components =  30 , neighbors =  7 , Score =  0.530864197531\n",
      "Components =  50 , neighbors =  1 , Score =  0.592592592593\n",
      "Components =  50 , neighbors =  2 , Score =  0.530864197531\n",
      "Components =  50 , neighbors =  3 , Score =  0.530864197531\n",
      "Components =  50 , neighbors =  4 , Score =  0.481481481481\n",
      "Components =  50 , neighbors =  5 , Score =  0.518518518519\n",
      "Components =  50 , neighbors =  6 , Score =  0.407407407407\n",
      "Components =  50 , neighbors =  7 , Score =  0.444444444444\n",
      "Components =  100 , neighbors =  1 , Score =  0.530864197531\n",
      "Components =  100 , neighbors =  2 , Score =  0.382716049383\n",
      "Components =  100 , neighbors =  3 , Score =  0.382716049383\n",
      "Components =  100 , neighbors =  4 , Score =  0.296296296296\n",
      "Components =  100 , neighbors =  5 , Score =  0.37037037037\n",
      "Components =  100 , neighbors =  6 , Score =  0.296296296296\n",
      "Components =  100 , neighbors =  7 , Score =  0.308641975309\n"
     ]
    }
   ],
   "source": [
    "#Let's tune the knn n_neighbors parameter\n",
    "neighbors = [1, 2, 3, 4, 5, 6, 7] #and using up to 7 neighbors\n",
    "\n",
    "#scores = np.zeros( (components[len(components)-1]+1, neighbors[len(neighbors)-1]+1 ) )\n",
    "\n",
    "#show PCA scores as a function of neighbors, N, and no. of components\n",
    "for component in components:\n",
    "    for n in neighbors:\n",
    "        knn = KNeighborsClassifier(n_neighbors=n)\n",
    "        knn.fit(standardized_X_pca[:,:component], y_train)\n",
    "        score = knn.score(standardized_X_test_pca[:,:component], y_test)\n",
    "        #predict = knn.predict(X_test_pca[:,:component])\n",
    "        #scores[component][n] = score\n",
    "        \n",
    "        print('Components = ', component, ', neighbors = ', n,', Score = ', score)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The highest score is 69% (Components =  20 , neighbors =  5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (V) Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Five machine learning models that are suitable for multilabel classification problems are assessed (i.e., DecisionTreeClassifier, ExtraTreeClassifier, ExtraTreesClassifier,  KNeighborsClassifier, and RandomForestClassifier).\n",
    "- Initial screening reveals that ExtraTreesClassifier and KNeighborsClassifier perform better than the three other algorithms. \n",
    "- Next, I utilize dimensionality reduction techniques (i.e., PCA) along with the two machine learning algorithms and optimize the parameters in the respective models. \n",
    "- The best score that is achieved is 69.0% (Components = 20 , neighbors = 5).  \n",
    "- To further improve the model accuracy, I will consider a neural network model.   \n",
    "- First, I will attempt a basic neural network algorithm to achieve an accuracy above 0.90. \n",
    "- If it is not achievable, I will consider employing a convolutional neural network algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
